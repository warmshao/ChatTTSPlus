MODELS:
  tokenizer:
    name: "Tokenizer"
    infer_type: "pytorch"
    kwargs:
      model_path: "checkpoints/asset/tokenizer.pt"
  dvae_encode:
    name: "DVAE"
    infer_type: "pytorch"
    kwargs:
      model_path: "checkpoints/asset/DVAE_full.pt"
      coef: ''
      dim: 512
      decoder_config:
        idim: 512
        odim: 512
        hidden: 256
        n_layer: 12
        bn_dim: 128
      encoder_config:
        idim: 512
        odim: 1024
        hidden: 256
        n_layer: 12
        bn_dim: 128
      vq_config:
        dim: 1024
        levels:
          - 5
          - 5
          - 5
          - 5
        G: 2
        R: 2
  dvae_decode:
    name: "DVAE"
    infer_type: "pytorch"
    kwargs:
      model_path: "checkpoints/asset/Decoder.pt"
      coef: ''
      dim: 384
      decoder_config:
        idim: 384
        odim: 384
        hidden: 512
        n_layer: 12
        bn_dim: 128
  gpt:
    name: "GPT"
    infer_type: "pytorch"
    kwargs:
      model_path: "checkpoints/asset/GPT.pt"
      gpt_config:
        hidden_size: 768
        intermediate_size: 3072
        num_attention_heads: 12
        num_hidden_layers: 20
        use_cache: False
        max_position_embeddings: 4096
        spk_emb_dim: 192
        spk_KL: False
        num_audio_tokens: 626
        num_vq: 4

DATA:
  train_bs: 4
  meta_infos:
    - "data/leijun/asr_opt/denoise_opt_new.list"
  sample_rate: 24000
  num_vq: 4

LORA:
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.01
  lora_target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
#    - "gate_proj"
#    - "up_proj"
#    - "down_proj"

solver:
  gradient_accumulation_steps: 1
  mixed_precision: 'fp16'
  gradient_checkpointing: false
  max_train_steps: 4000
  max_grad_norm: 1.0
  # lr
  learning_rate: 5e-5
  min_learning_rate: 1e-5
  scale_lr: false
  lr_warmup_steps: 10
  lr_scheduler: 'constant'

  # optimizer
  use_8bit_adam: false
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_weight_decay: 1e-3

weight_dtype: 'fp16'
output_dir: './outputs'
exp_name: "leijun_lora"
lora_model_path: ""
checkpointing_steps: 100
use_empty_speaker: true
